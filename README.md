MLOps Platform Prototipi ğŸš€
Bu proje, bir makine Ã¶ÄŸrenmesi modelinin eÄŸitiminden sunulmasÄ±na kadar olan tÃ¼m yaÅŸam dÃ¶ngÃ¼sÃ¼nÃ¼ kapsayan, uÃ§tan uca bir MLOps platformu prototipidir. Sistem, modern DevOps ve MLOps pratikleri kullanÄ±larak konteynerleÅŸtirilmiÅŸ servisler Ã¼zerine inÅŸa edilmiÅŸtir.

ğŸ›ï¸ Mimari ve Ä°ÅŸleyiÅŸ
Proje, Docker Compose ile yÃ¶netilen bir mikroservis mimarisine sahiptir. Temel iÅŸleyiÅŸ aÅŸaÄŸÄ±daki gibidir:

KullanÄ±cÄ± ArayÃ¼zÃ¼ (Frontend): KullanÄ±cÄ±, basit web arayÃ¼zÃ¼ Ã¼zerinden model eÄŸitimi veya tahmin talebinde bulunur.
API Sunucusu (Flask App): Ä°stekleri karÅŸÄ±layan ana uygulamadÄ±r.
/train isteÄŸi geldiÄŸinde, Iris veri seti Ã¼zerinde bir model eÄŸitir.
EÄŸitim parametrelerini ve metriklerini MLflow Tracking Server'a kaydeder.
EÄŸitilmiÅŸ model dosyasÄ±nÄ± (artifact) MinIO depolama servisine gÃ¶nderir.
/predict isteÄŸi geldiÄŸinde, belirtilen modeli MLflow ve MinIO Ã¼zerinden yÃ¼kler ve gelen veri iÃ§in tahmin yapar.
MLflow Server: TÃ¼m deneylerin (parametreler, metrikler, vb.) meta verilerini takip eden ve saklayan servistir. Modellerin nerede saklandÄ±ÄŸÄ± bilgisini tutar.
MinIO Server: EÄŸitilmiÅŸ modeller gibi bÃ¼yÃ¼k dosyalarÄ±n saklandÄ±ÄŸÄ± S3 uyumlu bir nesne depolama servisidir.
<!-- end list -->

Kod snippet'i

graph TD
    A[ğŸ‘¨â€ğŸ’» KullanÄ±cÄ±] -->|TarayÄ±cÄ±| B(ğŸŒ Frontend);
    B -->|API Ã‡aÄŸrÄ±sÄ±| C{ğŸš€ Flask API};
    C -->|Loglama| D[ğŸ“Š MLflow Server];
    C -->|Model YÃ¼kleme/Kaydetme| E[ğŸ“¦ MinIO Storage];
    D -->|Artifact Konumu| E;
âœ¨ Ã–zellikler
API ile Model EÄŸitimi: Tek bir GET isteÄŸi ile modelin eÄŸitilmesi ve versiyonlanmasÄ±.
API ile Tahmin: POST isteÄŸi ile daha Ã¶nce eÄŸitilmiÅŸ herhangi bir modelin kullanÄ±larak tahmin yapÄ±lmasÄ±.
Deney Takibi: MLflow ile tÃ¼m model eÄŸitimlerinin parametre ve metriklerinin takibi.
Model ve Artifact Depolama: MinIO ile eÄŸitilmiÅŸ modellerin kalÄ±cÄ± ve gÃ¼venli bir ÅŸekilde saklanmasÄ±.
Ä°nteraktif ArayÃ¼z: Modelleri eÄŸitmek ve test etmek iÃ§in basit ve kullanÄ±cÄ± dostu bir web arayÃ¼zÃ¼.
KonteynerleÅŸtirilmiÅŸ AltyapÄ±: TÃ¼m servislerin Docker ile izole edilmesi ve Docker Compose ile kolayca yÃ¶netilmesi.
ğŸ› ï¸ KullanÄ±lan Teknolojiler
Backend: Python, Flask, Gunicorn
MLOps & Veri: MLflow, Scikit-learn, Pandas
AltyapÄ± & KonteynerleÅŸtirme: Docker, Docker Compose
VeritabanÄ± & Depolama: MinIO (S3 Uyumlu), SQLite
Frontend: HTML, CSS, JavaScript (Fetch API)
ğŸš€ Kurulum ve Ã‡alÄ±ÅŸtÄ±rma
Projeyi yerel makinenizde Ã§alÄ±ÅŸtÄ±rmak iÃ§in aÅŸaÄŸÄ±daki adÄ±mlarÄ± izleyin:

Projeyi KlonlayÄ±n:

Bash

git clone https://github.com/mehmetyusufdemir/mlops-platform.git
cd mlops-platform
Docker Servislerini BaÅŸlatÄ±n:
Projenin ana dizinindeyken aÅŸaÄŸÄ±daki komutu Ã§alÄ±ÅŸtÄ±rÄ±n. Bu komut, gerekli imajlarÄ± indirecek, build edecek ve tÃ¼m servisleri baÅŸlatacaktÄ±r.

Bash

docker-compose up --build
ArayÃ¼zlere EriÅŸin:
Sistem tamamen ayaÄŸa kalktÄ±ktan sonra, aÅŸaÄŸÄ±daki adreslerden servislere ulaÅŸabilirsiniz:

MLOps Platform ArayÃ¼zÃ¼: http://localhost:5003
MLflow Paneli: http://localhost:5002
MinIO Konsolu: http://localhost:9001 (KullanÄ±cÄ± AdÄ±: minioadmin, Åifre: minioadmin)
ğŸ“– KullanÄ±m
http://localhost:5003 adresine gidin.
"Yeni Model EÄŸit" butonuna tÄ±klayÄ±n. "SonuÃ§lar" bÃ¶lÃ¼mÃ¼nde baÅŸarÄ±lÄ± bir yanÄ±t ve run_id gÃ¶receksiniz.
OluÅŸturulan run_id, tahmin bÃ¶lÃ¼mÃ¼ndeki "Run ID" kutusuna otomatik olarak dolacaktÄ±r.
Tahmin yapmak iÃ§in 4 adet sayÄ±sal Ã¶zellik girin ve "Tahmin Yap" butonuna tÄ±klayÄ±n.
Sonucu "SonuÃ§lar" bÃ¶lÃ¼mÃ¼nde gÃ¶rÃ¼n.
&lt;br>

MLOps Platform Prototype ğŸš€
This project is an end-to-end MLOps platform prototype that covers the entire lifecycle of a machine learning model, from training to serving. The system is built on containerized services using modern DevOps and MLOps practices.

ğŸ›ï¸ Architecture and Workflow
The project utilizes a microservice architecture managed by Docker Compose. The basic workflow is as follows:

User Interface (Frontend): The user initiates a model training or prediction request through a simple web interface.
API Server (Flask App): This is the main application that handles requests.
On a /train request, it trains a model on the Iris dataset.
It logs the training parameters and metrics to the MLflow Tracking Server.
It sends the trained model file (artifact) to the MinIO storage service.
On a /predict request, it loads the specified model from MLflow and MinIO and makes a prediction on the new data.
MLflow Server: A service that tracks and stores the metadata of all experiments (parameters, metrics, etc.). It holds the information about where the models are stored.
MinIO Server: An S3-compatible object storage service where large files, such as trained models, are stored.
<!-- end list -->

Kod snippet'i

graph TD
    A[ğŸ‘¨â€ğŸ’» User] -->|Browser| B(ğŸŒ Frontend);
    B -->|API Call| C{ğŸš€ Flask API};
    C -->|Logging| D[ğŸ“Š MLflow Server];
    C -->|Model Load/Save| E[ğŸ“¦ MinIO Storage];
    D -->|Artifact Location| E;
âœ¨ Features
API-Driven Model Training: Train and version a model with a single GET request.
API-Driven Prediction: Use any previously trained model to make predictions via a POST request.
Experiment Tracking: Track parameters and metrics of all training runs with MLflow.
Model & Artifact Storage: Persistently and securely store trained models with MinIO.
Interactive UI: A simple and user-friendly web interface to train and test models.
Containerized Infrastructure: All services are isolated with Docker and easily managed with Docker Compose.
ğŸ› ï¸ Tech Stack
Backend: Python, Flask, Gunicorn
MLOps & Data: MLflow, Scikit-learn, Pandas
Infrastructure & Containerization: Docker, Docker Compose
Database & Storage: MinIO (S3-Compatible), SQLite
Frontend: HTML, CSS, JavaScript (Fetch API)
ğŸš€ Setup and Running
Follow these steps to run the project on your local machine:

Clone the Repository:

Bash

git clone https://github.com/mehmetyusufdemir/mlops-platform.git
cd mlops-platform
Start Docker Services:
From the project's root directory, run the following command. This will download the necessary images, build the custom image, and start all services.

Bash

docker-compose up --build
Access the Interfaces:
After the system is fully up and running, you can access the services at the following addresses:

MLOps Platform UI: http://localhost:5003
MLflow Dashboard: http://localhost:5002
MinIO Console: http://localhost:9001 (Username: minioadmin, Password: minioadmin)
ğŸ“– Usage
Navigate to http://localhost:5003.
Click the "Yeni Model EÄŸit" (Train New Model) button. You will see a success response and a run_id in the results area.
The generated run_id will be auto-filled into the "Run ID" input box in the prediction section.
Enter 4 numerical features to make a prediction and click the "Tahmin Yap" (Make Prediction) button.
See the prediction result in the results area.
